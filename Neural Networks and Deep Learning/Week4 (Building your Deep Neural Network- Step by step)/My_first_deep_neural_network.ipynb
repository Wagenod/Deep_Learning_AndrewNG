{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0. How will work my Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<u>**TEXT:**</u>\n",
    "    1. Initialize neural network\n",
    "    2. Loop:\n",
    "        - forward propagation;\n",
    "        - compute cost;\n",
    "        - backward propagation;\n",
    "        - update parameters;\n",
    "    3. Use DNN to predict.\n",
    "<u>** GENERAL CODE OF MAIN FUNCTION:**</u>\n",
    "```python\n",
    "def L_nn(X,Y, layers_dims, learning_rate = ..., print_cost = ..., num_epochs = ...):\n",
    "        parameters = init_nn(layers_dims);  \n",
    "        ...\n",
    "        for epoch in range(num_epochs):\n",
    "            ...\n",
    "            AL, caches = model_forward(X,parameters); \n",
    "            ...\n",
    "            cost = compute_cost(AL,Y);     \n",
    "            ...\n",
    "            grads = model_backward(AL, Y, caches);   \n",
    "            ...\n",
    "            parameters = update_parameters(grads, learning_rate, parameters);\n",
    "            ...\n",
    "        return parameters; \n",
    "```\n",
    "Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from testCases_v4 import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Initialize neural network        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def init_nn(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layers_dims - turple of of size L (number nn layers). Each element  i.e layers_dims[i] is number\n",
    "        of neurans in layer i.\n",
    "        \n",
    "    Return:\n",
    "        parameters - dict weights of neural network ({\"W1\":[...],\"b1\":[...],...})  \n",
    "    \"\"\"\n",
    "    parameters = dict();\n",
    "    L = len (layers_dims) - 1;\n",
    "    \n",
    "    for l in range(1, L + 1): # l = 1,2,3...L\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*0.01;\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l],1));\n",
    "        \n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "            Z - weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "    \n",
    "    Returns:\n",
    "            Sigmoid value of Z\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "            Z- weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "            \n",
    "    Returns: \n",
    "            ReLu value of Z (max(0,Z))\n",
    "    \"\"\"\n",
    "    #print ('relu activation:')\n",
    "    #print(Z.shape)\n",
    "    #print (np.maximum(0,Z).shape)\n",
    "    return np.maximum(0,Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def divide_relu(Z):\n",
    "    Z[Z<0] = 0\n",
    "    Z[Z>0] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def divide_sigmoid(Z):\n",
    "    return sigmoid(Z)*(1 - sigmoid(Z));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W,A_prev) + b;\n",
    "    #print('linear forward:')\n",
    "    #print(A_prev.shape)\n",
    "    #print(W.shape)\n",
    "    #print(b.shape)\n",
    "    #print (Z.shape)\n",
    "    #print('-------')\n",
    "    cache = Z\n",
    "    return Z, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):    \n",
    "    Z, activation_cache = linear_forward(A_prev, W, b);\n",
    "    #print ('linear_activ_forw:')\n",
    "    #print(Z.shape)\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z);\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z);\n",
    "    #print (A.shape)\n",
    "    #print('------------')\n",
    "    linear_cache = (A_prev, W, b)\n",
    "    caches = (linear_cache, activation_cache)\n",
    "    return A, caches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "def model_forward(X,parameters): \n",
    "    caches = [];\n",
    "    L = len(parameters) // 2;\n",
    "    A_prev = X;\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        W = parameters[\"W\" + str(l)];\n",
    "        b = parameters[\"b\" + str(l)];\n",
    "        A_prev, current_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\");\n",
    "        caches.append(current_cache);\n",
    "     \n",
    "    AL, current_cache = linear_activation_forward(A_prev, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\" );\n",
    "    caches.append(current_cache)\n",
    "    return AL, caches;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1];\n",
    "    return -np.sum(Y*np.log(AL) + (1 - Y)*np.log(1 - AL))/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6. Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(linear_cache, dZ):\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.transpose()) / m;\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m;\n",
    "    dA_prev = np.dot(W.transpose(),dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(Z, dA):\n",
    "    return dA*divide_sigmoid(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(Z, dA):\n",
    "    dZ = dA * divide_relu(Z);\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(activation_cache, dA)\n",
    "        dA_prev, dW, db = linear_backward(linear_cache, dZ)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(activation_cache, dA)\n",
    "        dA_prev, dW, db = linear_backward(linear_cache, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches): \n",
    "    grads = dict();\n",
    "    L = len(caches)\n",
    "    \n",
    "    dAL = - ( np.divide(Y,AL) - np.divide(1 - Y, 1 - AL));\n",
    "    dA_prev = dAL\n",
    "    current_cache = caches[L-1] # 0,1,2,...,L-1\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(  dAL, current_cache, activation = \"sigmoid\");\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l-1];\n",
    "        grads[\"dA\" + str(l-1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = linear_activation_backward(grads['dA' + str(l)], current_cache,  activation = \"relu\")\n",
    "    return grads;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 7. Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def update_parameters (old_params, grads, learning_rate = 0.075):\n",
    "    \n",
    "    L = len(old_params) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        old_params['W' + str(l)] = old_params['W' + str(l)] - learning_rate*grads['dW' + str(l)];\n",
    "        old_params['b' + str(l)] = old_params['b' + str(l)] - learning_rate*grads['db' + str(l)];\n",
    "    \n",
    "    return old_params;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Join all components in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def my_DNN_model(X, Y, layers_dims, X_valid,Y_valid, learning_rate = 0.0075, num_epochs = 3000, print_cost = True):\n",
    "    \n",
    "    parameters = init_nn(layers_dims)\n",
    "    costs = dict()\n",
    "    costs['train'] = list()\n",
    "    costs['valid'] = list()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        AL, caches = model_forward(X=X,parameters=parameters)\n",
    "        cost_train = compute_cost(AL=AL, Y=Y)\n",
    "        #compute validation cost error\n",
    "        AL_valid, caches_valid = model_forward(X = X_valid, parameters=parameters)\n",
    "        cost_valid = compute_cost(AL=AL_valid,Y=Y_valid)\n",
    "    \n",
    "        if print_cost and epoch%100 == 0:\n",
    "            print (\"Cost after epoch \" + str(epoch) + \" : train = \" + str(cost_train) + \"; valid = \" + str(cost_valid) )\n",
    "            costs['train'].append(cost_train)\n",
    "            costs['valid'].append(cost_valid)\n",
    "            \n",
    "        grads = model_backward(AL=AL, Y=Y, caches=caches)\n",
    "        parameters = update_parameters(grads=grads,learning_rate=learning_rate, old_params=parameters)\n",
    "\n",
    "    plot_cost_errors(costs, num_epochs, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_cost_errors(costs, num_epochs, learning_rate, image_name_to_save = 'errors.png'):\n",
    "    \n",
    "    plt.plot(range(100,num_epochs + 100,100),np.squeeze(costs['train']))\n",
    "    plt.plot(range(100,num_epochs + 100,100), np.squeeze(costs['valid']))\n",
    "    \n",
    "    font = {'color':'black', 'fontname':'Arial', 'weight':'normal'}\n",
    "    \n",
    "    plt.ylabel('cost', size = 15, fontdict = font)\n",
    "    plt.xlabel('iterations (per tens)', size = 15, fontdict = font)\n",
    "             \n",
    "    plt.title(\"Learning rate =\" + str(learning_rate),  size = 20, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "             \n",
    "    plt.xticks(size = 12, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "    plt.yticks(size = 12, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "             \n",
    "    plt.xlim(100,num_epochs)\n",
    "    plt.legend(['Train error','Validation error'], loc = 'upper right')\n",
    "    plt.savefig(image_name_to_save, bbox_inches  = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    AL, caches = model_forward(X,parameters)\n",
    "    targets = (AL > 0.5)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    num_match = np.sum(y_pred*y_true) + np.sum((1-y_pred)*(1-y_true))\n",
    "    m = y_pred.shape[1]\n",
    "    return (num_match / m)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Using model for cats classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user_2018\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('../../../datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('../../../datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Exploratory dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GENERAL PROPERTIES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 209 train images\n",
      "We have 50 test images\n",
      "Each images size: (64, 64)\n"
     ]
    }
   ],
   "source": [
    "print (\"We have {} train images\".format(train_x_orig.shape[0]))\n",
    "print (\"We have {} test images\".format(test_x_orig.shape[0]))\n",
    "print (\"Each images size: {}\".format(train_x_orig[0].shape[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAPES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x_orig is (209, 64, 64, 3)\n",
      "Shape of train_y is (1, 209)\n",
      "Shape of test_x_orig is (50, 64, 64, 3)\n",
      "Shape of test_y is (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape of train_x_orig is {}\".format(train_x_orig.shape))\n",
    "print (\"Shape of train_y is {}\".format(train_y.shape))\n",
    "print (\"Shape of test_x_orig is {}\".format(test_x_orig.shape))\n",
    "print (\"Shape of test_y is {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHOW RANDOM PICTURE FROM TRAIN SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28b8eedf2e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = train_x_orig.shape[1];\n",
    "number = np.random.randint(0,m-1)\n",
    "plt.imshow(train_x_orig[number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.\n",
    "<img src=\"images/imvectorkiank.png\" style=\"width:500px;height:300px;\">\n",
    "<center> <u>Figure 1.</u> Image to vector conversion </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatting\n",
    "m_train = train_x_orig.shape[0] # number training examples\n",
    "m_test = test_x_orig.shape[0] #number test examples\n",
    "\n",
    "train_x_flatten = train_x_orig.reshape(m_train, 64*64*3).T\n",
    "test_x_flatten = test_x_orig.reshape(m_test, 64*64*3).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of training dataset: (12288, 209)\n",
      "New shape of test dataset: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "print (\"New shape of training dataset: {}\".format(train_x_flatten.shape))\n",
    "print (\"New shape of test dataset: {}\".format(test_x_flatten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "train_x_norm = train_x_flatten / 255.\n",
    "test_x_norm = test_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Cats classification with my_DNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.1 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = train_x_norm.shape[0]\n",
    "n_h = 1\n",
    "n_y = 1\n",
    "parameters = my_DNN_model(X = train_x_norm, Y = train_y, layers_dims = (n_x,n_h,n_y),X_valid = test_x_norm, Y_valid = test_y, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 9.5.2 Predict and compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targets_train = predict(X = train_x_norm, parameters=parameters)\n",
    "targets_test = predict(X = test_x_norm, parameters=parameters)\n",
    "print (\"Accuracy on training set: \" + str(accuracy(y_pred=targets_train, y_true=train_y)))\n",
    "print (\"Accuracy on test set: \" + str(accuracy(y_pred=targets_test, y_true=test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Union my model in one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     2,
     30,
     35,
     53,
     62,
     71,
     73,
     78,
     92,
     105,
     111,
     116,
     131,
     134,
     139,
     149,
     154,
     162,
     183
    ]
   },
   "outputs": [],
   "source": [
    "class my_DNN_model:\n",
    "    \n",
    "    def __init__ (self, layers_dims, learning_rate = 0.0075, num_epochs = 2000, print_cost = True):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers_dims = layers_dims\n",
    "        self.print_cost = print_cost\n",
    "        \n",
    "    def fit (self, x_train, y_train, x_valid, y_valid):\n",
    "        self.parameters = self.__init_parameters()\n",
    "        costs = dict()\n",
    "        costs['train'] = list()\n",
    "        costs['valid'] = list()\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            AL, caches = self.__model_forward(x_train)\n",
    "            cost_train = self.compute_cost(AL=AL, Y=y_train)\n",
    "            #compute validation cost error\n",
    "            AL_valid, caches_valid = self.__model_forward(X = x_valid)\n",
    "            cost_valid = self.compute_cost(AL=AL_valid,Y=y_valid)\n",
    "\n",
    "            if self.print_cost and epoch%100 == 0:\n",
    "                print (\"Cost after epoch \" + str(epoch) + \" : train = \" + str(cost_train) + \"; valid = \" + str(cost_valid) )\n",
    "                costs['train'].append(cost_train)\n",
    "                costs['valid'].append(cost_valid)\n",
    "\n",
    "            grads = self.__model_backward(AL=AL, Y=y_train, caches=caches)\n",
    "            self.parameters = self.__update_parameters(grads)\n",
    "\n",
    "        #self.__plot_cost_errors(costs)\n",
    "    def predict(self,x_test, threshold):\n",
    "        AL, caches = self.__model_forward(x_test)\n",
    "        targets = (AL > threshold)\n",
    "        return targets    \n",
    "    #network weights initiliazation\n",
    "    def __init_parameters(self):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layers_dims - turple of of size L (number nn layers). Each element  i.e layers_dims[i] is number\n",
    "            of neurans in layer i.\n",
    "\n",
    "        Return:\n",
    "            parameters - dict weights of neural network ({\"W1\":[...],\"b1\":[...],...})  \n",
    "        \"\"\"\n",
    "        parameters = dict();\n",
    "        L = len (self.layers_dims) - 1;\n",
    "\n",
    "        for l in range(1, L + 1): # l = 1,2,3...L\n",
    "            parameters['W' + str(l)] = np.random.randn(self.layers_dims[l],self.layers_dims[l-1])*0.01;\n",
    "            parameters['b' + str(l)] = np.zeros((self.layers_dims[l],1));\n",
    "\n",
    "        return parameters;\n",
    "    #function activations \n",
    "    def __relu(self, Z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "                Z- weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "\n",
    "        Returns: \n",
    "                ReLu value of Z (max(0,Z))\n",
    "        \"\"\"\n",
    "        return np.maximum(0,Z);\n",
    "    def __sigmoid(self,Z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "                Z - weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "\n",
    "        Returns:\n",
    "                Sigmoid value of Z\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    def __divide_sigmoid(self, Z):\n",
    "        return self.__sigmoid(Z)*(1 - self.__sigmoid(Z));\n",
    "    def __divide_relu(self, Z):\n",
    "        Z[Z<0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    #forward propagation\n",
    "    def __model_forward(self, X): \n",
    "        caches = [];\n",
    "        L = len(self.parameters) // 2;\n",
    "        A_prev = X;\n",
    "\n",
    "        for l in range(1,L):\n",
    "            W = self.parameters[\"W\" + str(l)];\n",
    "            b = self.parameters[\"b\" + str(l)];\n",
    "            A_prev, current_cache = self.__linear_activation_forward(A_prev, W, b, activation = \"relu\");\n",
    "            caches.append(current_cache);\n",
    "\n",
    "        AL, current_cache = self.__linear_activation_forward(A_prev, self.parameters[\"W\" + str(L)], self.parameters[\"b\" + str(L)], activation = \"sigmoid\" );\n",
    "        caches.append(current_cache)\n",
    "        return AL, caches;\n",
    "    def __linear_activation_forward(self, A_prev, W, b, activation):    \n",
    "        Z, activation_cache = self.__linear_forward(A_prev, W, b); #\n",
    "        #print ('linear_activ_forw:')\n",
    "        #print(Z.shape)\n",
    "        if activation == \"relu\":\n",
    "            A = self.__relu(Z);\n",
    "        elif activation == \"sigmoid\":\n",
    "            A = self.__sigmoid(Z);\n",
    "        #print (A.shape)\n",
    "        #print('------------')\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        caches = (linear_cache, activation_cache)\n",
    "        return A, caches;\n",
    "    def __linear_forward(self, A_prev, W, b):\n",
    "        Z = np.dot(W,A_prev) + b;\n",
    "        cache = Z\n",
    "        return Z, cache;\n",
    "    \n",
    "    #compute cost\n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1];\n",
    "        return -np.sum(Y*np.log(AL) + (1 - Y)*np.log(1 - AL))/m\n",
    "    \n",
    "    #backward propagation\n",
    "    def __model_backward(self, AL, Y, caches): \n",
    "        grads = dict();\n",
    "        L = len(caches)\n",
    "\n",
    "        dAL = - ( np.divide(Y,AL) - np.divide(1 - Y, 1 - AL));\n",
    "        dA_prev = dAL\n",
    "        current_cache = caches[L-1] # 0,1,2,...,L-1\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.__linear_activation_backward(  dAL, current_cache, activation = \"sigmoid\");\n",
    "        for l in range(L - 1, 0, -1):\n",
    "            current_cache = caches[l-1];\n",
    "            grads[\"dA\" + str(l-1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = self.__linear_activation_backward(grads['dA' + str(l)], current_cache,  activation = \"relu\")\n",
    "        return grads;\n",
    "    def __linear_activation_backward(self, dA, cache, activation):\n",
    "\n",
    "        linear_cache, activation_cache = cache\n",
    "        if activation == 'sigmoid':\n",
    "            dZ = self.__sigmoid_backward(activation_cache, dA)\n",
    "            dA_prev, dW, db = self.__linear_backward(linear_cache, dZ)\n",
    "        elif activation == 'relu':\n",
    "            dZ = self.__relu_backward(activation_cache, dA)\n",
    "            dA_prev, dW, db = self.__linear_backward(linear_cache, dZ)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    def __linear_backward(self, linear_cache, dZ):\n",
    "        A_prev, W, b = linear_cache\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = np.dot(dZ, A_prev.transpose()) / m;\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True) / m;\n",
    "        dA_prev = np.dot(W.transpose(),dZ)\n",
    "        return dA_prev, dW, db\n",
    "    def __sigmoid_backward(self, Z, dA):\n",
    "        return dA*self.__divide_sigmoid(Z);\n",
    "    def __relu_backward(self, Z, dA):\n",
    "        dZ = dA * self.__divide_relu(Z);\n",
    "        return dZ;\n",
    "    \n",
    "    #update parameters\n",
    "    def __update_parameters (self, grads):\n",
    "        parameters = self.parameters\n",
    "        L = len(parameters) // 2\n",
    "        for l in range(1, L + 1):\n",
    "            parameters['W' + str(l)] = parameters['W' + str(l)] - self.learning_rate*grads['dW' + str(l)];\n",
    "            parameters['b' + str(l)] = parameters['b' + str(l)] - self.learning_rate*grads['db' + str(l)];\n",
    "        return parameters;\n",
    "    #plot costs value\n",
    "    def __plot_cost_errors_plotly(self, costs):\n",
    "        # создаем линию для числа проданных копий\n",
    "        trace0 = go.Scatter(\n",
    "            x = np.arange(100,self.num_epochs + 100,100),\n",
    "            y = np.squeeze(costs['train']),\n",
    "            name='train error'\n",
    "        )\n",
    "        \n",
    "        trace1 = go.Scatter(\n",
    "            x = np.arange(100,self.num_epochs + 100,100),\n",
    "            y = np.squeeze(costs['valid']),\n",
    "            name='test error'\n",
    "        )\n",
    "        # определяем массив данных и задаем title графика в layout\n",
    "        data = [trace0, trace1]\n",
    "        layout = {'title': 'LogLoss value'}\n",
    "\n",
    "        # cоздаем объект Figure и визуализируем его\n",
    "        fig = go.Figure(data=data, layout=layout)\n",
    "        iplot(fig, show_link=False)\n",
    "        \n",
    "    def __plot_cost_errors(self, costs, image_name_to_save = 'errors.png'):\n",
    "    \n",
    "        plt.plot(range(100,self.num_epochs + 100,100),np.squeeze(costs['train']))\n",
    "        plt.plot(range(100,self.num_epochs + 100,100), np.squeeze(costs['valid']))\n",
    "\n",
    "        font = {'color':'black', 'fontname':'Arial', 'weight':'normal'}\n",
    "\n",
    "        plt.ylabel('cost', size = 15, fontdict = font)\n",
    "        plt.xlabel('iterations (per tens)', size = 15, fontdict = font)\n",
    "\n",
    "        plt.title(\"Learning rate =\" + str(self.learning_rate),  size = 20, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "\n",
    "        plt.xticks(size = 12, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "        plt.yticks(size = 12, color = 'black', fontname = 'Arial', weight = 'normal')\n",
    "\n",
    "        plt.xlim(100,self.num_epochs)\n",
    "        plt.legend(['Train error','Validation error'], loc = 'upper right')\n",
    "        plt.savefig(image_name_to_save, bbox_inches  = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_x = train_x_norm.shape[0]\n",
    "n_h = 1\n",
    "n_y = 1\n",
    "model = my_DNN_model((n_x,n_h,n_y),num_epochs=1300)\n",
    "model.fit(train_x_norm, train_y, test_x_norm, test_y)\n",
    "targets = model.predict(test_x_norm, 0.6)\n",
    "print (\"Accuracy on test set: \" + str(accuracy_score(y_pred=targets, y_true=test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def my_recall_score(y_pred, y_true):\n",
    "    return np.sum(y_pred*y_true)/np.sum(y_true);\n",
    "def FPR_def(y_pred, y_true):\n",
    "    FP = np.sum((1-y_true)*y_pred)\n",
    "    return FP/np.sum(1 - y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TPR - Recall for positiv class\n",
    "\n",
    "TPR = list ()\n",
    "for threshold in np.arange(0,1.05,0.05):\n",
    "    targets = model.predict(test_x_norm,threshold=threshold)\n",
    "    recall = my_recall_score(y_pred = targets,y_true=test_y)\n",
    "    TPR.append(recall)\n",
    "plt.plot(np.arange(0,1.05,0.05), TPR)\n",
    "font = {'color':'black', 'fontname':'Arial', 'weight':'normal', 'size':'15'}\n",
    "plt.xlabel('Threshold', fontdict = font)\n",
    "plt.ylabel('TPR', fontdict = font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FPR\n",
    "# TPR - Recall for positiv class\n",
    "\n",
    "FPR = list ()\n",
    "\n",
    "for threshold in np.arange(0,1.05,0.05):\n",
    "    targets = model.predict(test_x_norm,threshold=threshold)\n",
    "    recall = FPR_def(y_pred = targets,y_true=test_y)\n",
    "    FPR.append(recall)\n",
    "plt.plot(np.arange(0,1.05,0.05), FPR)\n",
    "font = {'color':'black', 'fontname':'Arial', 'weight':'normal', 'size':'15'}\n",
    "plt.xlabel('Threshold', fontdict = font)\n",
    "plt.ylabel('FPR', fontdict = font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# join\n",
    "plt.plot(np.arange(0,1.05,0.05), FPR)\n",
    "plt.plot(np.arange(0,1.05,0.05), TPR)\n",
    "font = {'color':'black', 'fontname':'Arial', 'weight':'normal', 'size':'15'}\n",
    "plt.xlabel('Threshold', fontdict = font)\n",
    "plt.ylabel('FPR/TPR', fontdict = font)\n",
    "plt.legend(['FPR','TPR'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# precision \n",
    "def my_precision_score(y_pred, y_true):\n",
    "    return np.sum(y_pred*y_true)/np.sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# precision(threshold)\n",
    "precision = list ()\n",
    "for threshold in np.arange(0,1.05,0.05):\n",
    "    targets = model.predict(test_x_norm,threshold=threshold)\n",
    "    prec = my_precision_score(y_pred = targets,y_true=test_y)\n",
    "    precision.append(prec)\n",
    "plt.plot(np.arange(0,1.05,0.05), precision)\n",
    "font = {'color':'black', 'fontname':'Arial', 'weight':'normal', 'size':'15'}\n",
    "plt.xlabel('Threshold', fontdict = font)\n",
    "plt.ylabel('Precision', fontdict = font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ploting roc-auc curve\n",
    "plt.plot(np.array(FPR), np.array(TPR))\n",
    "plt.plot(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1), linestyle = '--', color = 'red')\n",
    "font = {'color':'black', 'fontname':'Arial', 'weight':'normal', 'size':'15'}\n",
    "plt.xlabel('FPR', fontdict = font)\n",
    "plt.ylabel('TPR', fontdict = font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "#actual = [1,1,1,0,0,0]\n",
    "#predictions = [0.9,0.9,0.9,0.1,0.1,0.1]\n",
    "#false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predictions)\n",
    "#roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc = auc(FPR, TPR)\n",
    "print (roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class activations:\n",
    "    @staticmethod\n",
    "    def relu(Z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "                Z- weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "\n",
    "        Returns: \n",
    "                ReLu value of Z (max(0,Z))\n",
    "        \"\"\"\n",
    "        return np.maximum(0,Z);\n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "                Z - weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "\n",
    "        Returns:\n",
    "                Sigmoid value of Z\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "    @staticmethod\n",
    "    def divide_sigmoid(Z):\n",
    "        return activations.sigmoid(Z)*(1 - activations.sigmoid(Z));\n",
    "    @staticmethod\n",
    "    def divide_relu(Z):\n",
    "        Z[Z<0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
