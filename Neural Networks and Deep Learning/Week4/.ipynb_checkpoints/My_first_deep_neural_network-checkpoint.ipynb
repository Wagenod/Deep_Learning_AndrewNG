{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 0. How will work my Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<u>**TEXT:**</u>\n",
    "    1. Initialize neural network\n",
    "    2. Loop:\n",
    "        - forward propagation;\n",
    "        - compute cost;\n",
    "        - backward propagation;\n",
    "        - update parameters;\n",
    "    3. Use DNN to predict.\n",
    "<u>** GENERAL CODE OF MAIN FUNCTION:**</u>\n",
    "```python\n",
    "def L_nn(X,Y, layers_dims, learning_rate = ..., print_cost = ..., num_iterations = ...):\n",
    "        parameters = init_nn(layers_dims);  \n",
    "        ...\n",
    "        for epoch in range(num_iterations):\n",
    "            ...\n",
    "            AL, caches = model_forward(X,parameters); \n",
    "            ...\n",
    "            cost = compute_cost(AL,Y);     \n",
    "            ...\n",
    "            grads = model_backward(AL, Y, caches);   \n",
    "            ...\n",
    "            parameters = update_parameters(grads, learning_rate, parameters);\n",
    "            ...\n",
    "        return parameters; \n",
    "```\n",
    "Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from testCases_v4 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Initialize neural network        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     2,
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def init_nn(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        layers_dims - turple of of size L (number nn layers). Each element  i.e layers_dims[i] is number\n",
    "        of neurans in layer i.\n",
    "        \n",
    "    Return:\n",
    "        parameters - dict weights of neural network ({\"W1\":[...],\"b1\":[...],...})  \n",
    "    \"\"\"\n",
    "    parameters = dict();\n",
    "    L = len (layers_dims) - 1;\n",
    "    \n",
    "    for l in range(1, L + 1): # l = 1,2,3...L\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*0.01;\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l]));\n",
    "        \n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "            Z - weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "    \n",
    "    Returns:\n",
    "            Sigmoid value of Z\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "            Z- weighted sum of input for corresponding layer, i.e. for layer 2 Z2 = np.dot(W2,A1) + b2;\n",
    "            \n",
    "    Returns: \n",
    "            ReLu value of Z (max(0,Z))\n",
    "    \"\"\"\n",
    "    return np.maximum(0,Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    cache = [];\n",
    "    Z = np.dot(W,A_prev) + b;\n",
    "    cache.append(Z);\n",
    "    return Z, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    caches = dict();\n",
    "    \n",
    "    Z, activation_cache = linear_forward(A_prev, W, b);\n",
    "    caches[\"activation_cache\"] = activation_cache;\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z);\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z);\n",
    "        \n",
    "    caches[\"linear_cache\"] = {\"W\": W, \"A_prev\": A_prev, \"b\": b}\n",
    "    return A, caches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def model_forward(X,parameters):\n",
    "    \n",
    "    caches = [];\n",
    "    L = len(parameters) // 2;\n",
    "    A_prev = X;\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        W = parameters[\"W\" + str(l)];\n",
    "        b = parameters[\"b\" + str(l)];\n",
    "        A_prev, current_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\");\n",
    "        caches.append(current_cache);\n",
    "     \n",
    "    AL, current_cache = linear_activation_forward(A_prev, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\" );\n",
    "    caches.append(current_cache)\n",
    "    return AL, caches;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1];\n",
    "    return -np.sum(Y*np.log(AL) + (1 - Y)*np.log(1 - AL))/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(cache, dA, activation):\n",
    "    \n",
    "    \n",
    "    linear_cache = cache['linear_cache']\n",
    "    activation_cache = cache['activation_cache']\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(activation_cache, dA)\n",
    "        dA_prev, dW, db = linear_backward(linear_cache, dZ)\n",
    "    else if activation == 'relu':\n",
    "        dZ = relu_backward(activation_cache, dA)\n",
    "        dA_prev, dW, db = linear_backward(linear_cache, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(linear_cache, dZ):\n",
    "    m = dZ.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, linear_cache['A_prev'].transpose()) / m;\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m;\n",
    "    dA_prev = np.dot(linear_cache['dW'].transpose(),dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(activation_cache, dA):\n",
    "    \n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward():\n",
    "    \n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = dict();\n",
    "    L = len(caches)\n",
    "    \n",
    "    dAL = - ( np.divide(Y,AL) - np.deivide(1 - Y, 1 - AL));\n",
    "    dA_prev = dAL\n",
    "    current_cache = caches[L-1] # 0,1,2,...,L-1\n",
    "    grads[\"dW\" + str(L)], grads[\"db\" + str(L)], grads[\"dA\" + (L-1)] = linear_activation_backward(current_cache, dAL, activation = \"sigmoid\");\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l-1];\n",
    "        grads[\"dA\" + str(l-1)], grads[\"dW\" + str(l)], grads[\"db\" + str(l)] = linear_activation_backward(current_cache, grads['dA' + str(l)], activation = \"relu\")\n",
    "    return grads;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(5-1)):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in range(5,0,-1):\n",
    "    print (l);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
